#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
demo_q1_streamlit.py

Web UI ƒë∆°n gi·∫£n ƒë·ªÉ demo m√¥ h√¨nh SNGP cho q1_0:

- Cho ph√©p ch·ªçn param0 (site) v√† param1 (tag) t·ª´ exec_latencies.csv
- Model d·ª± ƒëo√°n plan t·ªët ‚Üí so s√°nh v·ªõi:
    + plan default
    + plan t·ªëi ∆∞u th·ª±c t·∫ø (min latency)

Ch·∫°y:
  streamlit run scripts/demo_q1_streamlit.py
"""

from __future__ import annotations

from typing import Any, Dict, List
import json
import os
import sys

import numpy as np
import pandas as pd
import streamlit as st
import tensorflow as tf

CURRENT = os.path.dirname(os.path.abspath(__file__))
ROOT = os.path.dirname(CURRENT)
if ROOT not in sys.path:
    sys.path.insert(0, ROOT)

from sngp_pipeline.models import ModelConfig, SNGPMultiheadModel

JSON = Any

# ================== CONFIG ƒê∆Ø·ªúNG D·∫™N ==================
LAT_PATH = "data/exec_latencies.csv"          # file b·∫°n ƒë√£ convert
MODEL_DIR = "models/sngp_nearopt_q1_0"        # th∆∞ m·ª•c khi train

METADATA_PATH   = os.path.join(MODEL_DIR, "metadata.json")
PLAN_COVER_PATH = os.path.join(MODEL_DIR, "plan_cover.json")
WEIGHTS_PATH    = os.path.join(MODEL_DIR, "model.weights.h5")


# ================== H√ÄM PH·ª§ ==================
def build_preprocessing_config(
    metadata: Dict[str, Any],
    exec_df: pd.DataFrame,
) -> List[Dict[str, Any]]:
    """Sinh preprocessing_config t∆∞∆°ng ·ª©ng v·ªõi t·ª´ng predicate."""
    predicates = metadata.get("predicates", [])
    configs: List[Dict[str, Any]] = []

    for i, pred in enumerate(predicates):
        dtype = pred.get("data_type")
        col = f"param{i}"

        if dtype == "float":
            if col not in exec_df.columns:
                raise ValueError(f"Thi·∫øu c·ªôt {col} trong exec_latencies.csv")
            series = exec_df[col].astype(float)
            mean = float(series.mean())
            var = float(series.var())
            if var <= 0:
                var = 1.0
            configs.append(
                {
                    "type": "std_normalization",
                    "mean": mean,
                    "variance": var,
                }
            )
        elif dtype == "int":
            if col not in exec_df.columns:
                raise ValueError(f"Thi·∫øu c·ªôt {col} trong exec_latencies.csv")
            series = exec_df[col].astype(int)
            min_v = int(series.min())
            max_v = int(series.max())
            pred["min"] = min_v
            pred["max"] = max_v
            configs.append(
                {
                    "type": "one_hot",
                }
            )
        else:
            # text: ƒë√£ c√≥ distinct_values trong metadata (do l√∫c train ƒë√£ enrich)
            if "distinct_values" not in pred:
                raise ValueError(
                    f"Predicate text {pred.get('name')} "
                    "ch∆∞a c√≥ distinct_values trong metadata.json"
                )
            configs.append(
                {
                    "type": "embedding",
                    "output_dim": 16,
                    "num_oov_indices": 1,
                }
            )

    if len(configs) != len(predicates):
        raise ValueError(
            "ƒê·ªô d√†i preprocessing_config kh√¥ng kh·ªõp v·ªõi s·ªë predicate "
            f"({len(configs)} vs {len(predicates)})"
        )

    return configs


def build_model_config(num_plans: int) -> ModelConfig:
    """Gi·ªëng file train_sngp_nearopt.py."""
    layer_sizes = [64, 64]
    dropout_rates = [0.1, 0.1]
    return ModelConfig(
        layer_sizes=layer_sizes,
        dropout_rates=dropout_rates,
        learning_rate=1e-3,
        activation="relu",
        loss="binary_crossentropy",
        metrics=[tf.keras.metrics.BinaryAccuracy(name="bin_acc")],
        spectral_norm_multiplier=0.9,
        num_gp_random_features=128,
    )


def build_vocab_maps(metadata: Dict[str, Any]) -> Dict[int, Dict[str, int]]:
    """T·∫°o map: index predicate -> {value_string -> id} (gi·ªëng trainer)."""
    vocab_maps: Dict[int, Dict[str, int]] = {}
    for idx, pred in enumerate(metadata.get("predicates", [])):
        if pred.get("data_type") == "text":
            vocab = pred.get("distinct_values", [])
            vocab_maps[idx] = {str(v): i for i, v in enumerate(vocab)}
    return vocab_maps


def encode_params_to_inputs(
    params: List[Any],
    metadata: Dict[str, Any],
    vocab_maps: Dict[int, Dict[str, int]],
) -> List[np.ndarray]:
    """Encode list param values -> list input tensor (gi·ªëng construct_training_data)."""
    X: List[np.ndarray] = []
    predicates = metadata.get("predicates", [])

    for i, val in enumerate(params):
        pred = predicates[i]
        dtype = pred.get("data_type")

        if dtype == "int":
            arr = np.asarray([[int(val)]], dtype=np.int64)
            X.append(arr)

        elif dtype == "float":
            arr = np.asarray([[float(val)]], dtype=np.float32)
            X.append(arr)

        elif dtype == "text":
            vm = vocab_maps.get(i, {})
            s = "" if val is None else str(val)
            idx_id = vm.get(s, 0)  # n·∫øu kh√¥ng c√≥ trong vocab, map v·ªÅ 0
            arr = np.asarray([[idx_id]], dtype=np.int64)
            X.append(arr)
        else:
            raise ValueError(f"Unsupported data_type for features: {dtype}")

    return X


# ================== CACHE T·∫¢I MODEL & DATA ==================
@st.cache_resource
def load_all():
    # 1) Exec latencies
    exec_df = pd.read_csv(LAT_PATH)
    sort_cols = [c for c in ["param0", "param1", "plan_id"] if c in exec_df.columns]
    if sort_cols:
        exec_df = exec_df.sort_values(by=sort_cols).reset_index(drop=True)

    # 2) metadata + plan_cover
    with open(METADATA_PATH, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    with open(PLAN_COVER_PATH, "r", encoding="utf-8") as f:
        plan_cover = json.load(f)

    # 3) preprocessing_config + model_config
    preprocessing_config = build_preprocessing_config(metadata, exec_df)
    model_config = build_model_config(num_plans=len(plan_cover))

    # 4) x√¢y d·ª±ng model SNGP + load weights
    model = SNGPMultiheadModel(
        metadata=metadata,
        plan_ids=plan_cover,
        model_config=model_config,
        preprocessing_config=preprocessing_config,
    )
    keras_model = model.get_model()

    # Load weights m·ªÅm (ph√≤ng khi l·ªách ch√∫t)
    keras_model.load_weights(
        WEIGHTS_PATH,
        by_name=True,
        skip_mismatch=True,
    )

    # 5) vocab maps
    vocab_maps = build_vocab_maps(metadata)

    return exec_df, metadata, plan_cover, keras_model, vocab_maps


# ================== STREAMLIT APP ==================
def main():
    st.set_page_config(
        page_title="Kepler q1_0 SNGP Demo",
        layout="wide",
    )

    st.title("üîç Kepler SNGP Demo ‚Äì Query q1_0 (StackOverflow Tags)")
    st.write(
        "Demo nh·ªè: ch·ªçn **(site, tag)** ‚Üí model ƒë·ªÅ xu·∫•t **plan** g·∫ßn t·ªëi ∆∞u, "
        "so s√°nh v·ªõi plan **default** v√† **t·ªëi ∆∞u th·ª±c t·∫ø** (min latency)."
    )

    # Load m·ªçi th·ª© (ƒë√£ cache)
    exec_df, metadata, plan_cover, keras_model, vocab_maps = load_all()

    # ====== L·∫•y danh s√°ch param0 / param1 t·ª´ exec_latencies.csv ======
    all_sites = sorted(exec_df["param0"].unique().tolist())
    default_site = "stackoverflow" if "stackoverflow" in all_sites else all_sites[0]

    col_left, col_right = st.columns(2)

    # 1. Ch·ªçn tham s·ªë (param)
    with col_left:
        st.header("1Ô∏è‚É£ Ch·ªçn tham s·ªë (param) ‚Üî")

        # Ch·ªçn site tr∆∞·ªõc
        site = st.selectbox(
            "param0 (site)",
            all_sites,
            index=all_sites.index(default_site) if default_site in all_sites else 0,
        )

        # Sau khi ch·ªçn site ‚Üí ch·ªâ l·∫•y tag c√≥ data cho site ƒë√≥
        tags_for_site = sorted(
            exec_df.loc[exec_df["param0"] == site, "param1"].unique().tolist()
        )
        tag = st.selectbox("param1 (tag)", tags_for_site)

        # N√∫t predict
        predict_btn = st.button("üîç D·ª± ƒëo√°n plan & so s√°nh")

    with col_right:
        st.subheader("‚ÑπÔ∏è Th√¥ng tin model & d·ªØ li·ªáu")
        st.markdown(
            f"""
            - **S·ªë d√≤ng exec_latencies**: `{len(exec_df):,}`
            - **S·ªë plan trong plan_cover**: `{len(plan_cover)}`
            - **Query ID**: `{metadata.get("query_id", "q1_0")}`
            """
        )

    st.markdown("---")

    if not predict_btn:
        return

    # ================== PREDICT ==================
    # L·ªçc ra t·∫•t c·∫£ d√≤ng t∆∞∆°ng ·ª©ng (site, tag) trong exec_latencies
    mask = (exec_df["param0"] == site) & (exec_df["param1"] == tag)
    df_key = exec_df[mask].copy()

    if df_key.empty:
        st.error("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu th·ª±c nghi·ªám cho c·∫∑p param ƒë√£ ch·ªçn.")
        return

    # T√≠nh plan t·ªëi ∆∞u th·ª±c t·∫ø (min latency)
    best_idx = df_key["latency_ms"].idxmin()
    best_row = df_key.loc[best_idx]
    best_plan_id = int(best_row["plan_id"])
    best_latency = float(best_row["latency_ms"])

    # L·∫•y plan default (is_default == True)
    df_default = df_key[df_key["is_default"] == True]
    if df_default.empty:
        default_plan_id = None
        default_latency = None
    else:
        def_row = df_default.iloc[0]
        default_plan_id = int(def_row["plan_id"])
        default_latency = float(def_row["latency_ms"])

    # Encode input cho model
    params = [site, tag]
    X_list = encode_params_to_inputs(params, metadata, vocab_maps)

    # Predict (SNGP model tr·∫£ v·ªÅ [logits, covariance])
    logits, covariance = keras_model.predict(X_list, verbose=0)
    scores = logits[0]  # (num_plans,)

    # Plan model ch·ªçn (theo score cao nh·∫•t)
    model_idx = int(np.argmax(scores))
    model_plan_id = int(plan_cover[model_idx])

    df_model = df_key[df_key["plan_id"] == model_plan_id]
    if df_model.empty:
        model_latency = None
    else:
        model_latency = float(df_model["latency_ms"].iloc[0])

    # ============== HI·ªÇN TH·ªä T√ìM T·∫ÆT ==============
    c1, c2, c3 = st.columns(3)

    c1.metric(
        "‚úÖ Plan t·ªëi ∆∞u (th·ª±c nghi·ªám)",
        f"plan {best_plan_id}",
        f"{best_latency:.3f} ms",
    )

    if default_plan_id is not None and default_latency is not None:
        c2.metric(
            "‚öôÔ∏è Plan default (optimizer)",
            f"plan {default_plan_id}",
            f"{default_latency:.3f} ms",
        )
    else:
        c2.write("‚öôÔ∏è Kh√¥ng c√≥ plan default trong d·ªØ li·ªáu.")

    if model_latency is not None:
        # So s√°nh v·ªõi default & optimal
        if default_latency is not None and model_latency > 0:
            speedup_vs_default = default_latency / model_latency
            delta_vs_default = f"{speedup_vs_default:.2f}√ó nhanh h∆°n default"
        else:
            delta_vs_default = "N/A"

        if best_latency > 0:
            slow_vs_best = model_latency / best_latency
            delta_vs_best = f"{slow_vs_best:.2f}√ó ch·∫≠m h∆°n t·ªëi ∆∞u"
        else:
            delta_vs_best = "N/A"

        c3.metric(
            "üß† Plan model ƒë·ªÅ xu·∫•t",
            f"plan {model_plan_id}",
            f"{model_latency:.3f} ms",
        )

        st.success(
            f"**Model ch·ªçn plan `{model_plan_id}`** ‚Äì "
            f"latency ‚âà **{model_latency:.3f} ms**.\n\n"
            f"- So v·ªõi **default**: {delta_vs_default}\n"
            f"- So v·ªõi **t·ªëi ∆∞u**: {delta_vs_best}"
        )
    else:
        c3.write("üß† Model ch·ªçn plan kh√¥ng c√≥ trong exec_latencies (kh√¥ng ƒëo).")

    st.markdown("---")


if __name__ == "__main__":
    main()
